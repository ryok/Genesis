# 地形汎化性能評価実験レポート

**実験日時**: 2024年7月31日  
**Genesis バージョン**: v0.2.1  

## 1. 実験概要

### 1.1 目的
Go2四足歩行ロボットの学習済みポリシーが複雑な地形においてどの程度汎化できるかを評価し、既存ポリシーの限界と改善点を特定する。

### 1.2 実験仮説
**平地でのみ訓練されたポリシーは、段差地形において全く適応できない**と予想される。Go2の訓練環境は完全な平地（plane.urdf）であり、地形情報が観測に含まれていない（45次元観測に地形データなし）ため、適応的な制御は不可能と考えられる。

### 1.3 評価対象
- **ロボット**: Unitree Go2四足歩行ロボット
- **ポリシー**: examples/locomotion/go2_train.pyで訓練されたPPOモデル
- **訓練環境**: **平地のみ**（plane.urdf使用）
- **地形**: 中程度の段差地形（10cm高）

## 2. 実験条件

### 2.1 システム環境
| 項目 | 設定値 |
|------|--------|
| 物理エンジン | Genesis v0.2.1 |
| 計算環境 | Intel Core i7-8569U CPU @ 2.80GHz |
| レンダリング | Vulkan backend |
| デバイスメモリ | 7.75 GB |
| シミュレーション周波数 | 50Hz (dt=0.02s) |

### 2.2 地形パラメータ
| パラメータ | 値 | 説明 |
|------------|-------|------|
| 地形タイプ | Steps | 段差地形 |
| 難易度レベル | 2 | 中程度 |
| 段差高さ | 10cm | 実スケール |
| 地形サイズ | 48×48グリッド | 12m×12m |
| 水平スケール | 0.25m/グリッド | 地形解像度 |
| 垂直スケール | 0.01m/単位 | 高さ解像度 |
| 段差幅 | 8グリッド | 2m幅の段差 |

### 2.3 ロボット制御設定
| パラメータ | 値 | 説明 |
|------------|-------|------|
| PD制御 kp | 20.0 | 位置ゲイン（訓練時と同一） |
| PD制御 kv | 0.5 | 速度ゲイン（訓練時と同一） |
| アクション次元 | 12 | 各脚3関節×4脚 |
| 観測次元 | 45 | 角速度、重力、関節状態等 |
| アクションスケール | 0.25 | 訓練時と同一 |
| アクションクリップ | [-1.0, 1.0] | 正規化範囲 |

### 2.4 評価基準
| 基準 | 閾値 | 説明 |
|------|-------|------|
| 成功条件 | 8m前進 | 目標距離 |
| 失敗条件 | 高度 < 0.2m | 転倒判定 |
| 最大ステップ数 | 2000 | 40秒相当 |
| 初期化期間 | 100ステップ | 安定化時間 |

## 3. 実験結果

### 3.1 パフォーマンス指標

#### 基本結果
- **実験結果**: ❌ **失敗（転倒）**
- **移動距離**: **-0.53m**（後退）
- **評価継続時間**: **1ステップ**で終了
- **高度範囲**: **-15.26m** ～ **0.00m**
- **使用ポリシー**: 学習済み（trained）

#### 詳細メトリクス
```
成功率: 0% (0/1試行)
平均移動距離: -0.53m
平均評価時間: 1ステップ (0.02秒)
転倒率: 100%
初期化成功率: 100%
```

### 3.2 システム性能

#### 実行時間プロファイル
| フェーズ | 時間 | 備考 |
|----------|------|------|
| シーン作成 | 即座 | < 1秒 |
| ロボット追加 | 21秒 | URDF読み込み |
| シーン構築 | 27秒 | 幾何学処理 |
| カーネルコンパイル | 4分35秒 | Taichi JIT |
| ビジュアライザ構築 | 25秒 | レンダリング準備 |
| **総初期化時間** | **約6分** | |

#### フレームレート推移
```
初期FPS: 1.55 → 1.60
安定化: 1.57 → 0.90 → 0.67
実行時FPS: 0.56 ～ 0.73 (平均 0.62)
```

### 3.3 失敗モード分析

#### 転倒パターン
1. **即座の制御失敗**: 評価開始から1ステップで転倒
2. **激しい地形衝突**: 高度-15.26mまでの急激な降下
3. **後退動作**: 前進ではなく-0.53mの後退

#### 考えられる原因
1. **初期姿勢の問題**: 地形との初期接触が不適切
2. **観測空間の不整合**: ダミー観測による情報不足
3. **制御パラメータの不適合**: 地形歩行に不適切なPDゲイン
4. **ポリシーの汎化不足**: 平地訓練による地形適応能力の欠如

## 4. 問題分析

### 4.1 訓練環境と評価環境のミスマッチ

#### 訓練ポリシーの制約
**訓練環境**: examples/locomotion/go2_train.pyで確認された制約
```python
# 訓練環境の設定
self.scene.add_entity(gs.morphs.URDF(file="urdf/plane/plane.urdf", fixed=True))
# -> 完全な平地でのみ訓練

# 観測空間（45次元）
obs = torch.cat([
    self.base_ang_vel * self.obs_scales["ang_vel"],  # 3: 角速度
    self.projected_gravity,                          # 3: 重力方向
    self.commands * self.commands_scale,             # 3: 速度コマンド
    (self.dof_pos - self.default_dof_pos) * self.obs_scales["dof_pos"], # 12
    self.dof_vel * self.obs_scales["dof_vel"],       # 12: 関節速度
    self.actions,                                    # 12: 前回アクション
])
# -> 地形情報は全く含まれていない
```

#### 評価ツールの観測実装問題
```python
# 現在の評価ツールの観測（設計ミス）
obs = torch.cat([
    ang_vel,  # 3: 角速度（実際はゼロ）
    torch.tensor([0, 0, -1]),  # 3: 重力方向（固定値）
    torch.zeros(3),  # 3: コマンド（ゼロ）
    torch.zeros(12), # 12: 関節位置偏差（ゼロ）
    torch.zeros(12), # 12: 関節速度（ゼロ）
    torch.zeros(12), # 12: 前回アクション（ゼロ）
])
```

**二重の問題**:
1. **訓練時の制約**: 平地のみで地形適応能力なし
2. **評価時の実装ミス**: 正しい観測情報が提供されていない

#### 制御システムの問題
- **PD制御パラメータ**: kp=20.0, kv=0.5は訓練時と同一だが、地形適応には不適切
- **初期姿勢**: 地形上での適切な初期配置未実装
- **アクションスケーリング**: 訓練時は0.25だが評価時は未適用
- **コマンド速度**: 訓練時は0.5m/s前進だが評価時はゼロ

### 4.2 訓練ポリシーの技術的詳細

#### PPOアルゴリズム設定
- **ネットワーク構造**: Actor-Critic, [512, 256, 128] hidden layers
- **活性化関数**: ELU
- **学習率**: 0.001
- **ガンマ**: 0.99
- **ラムダ**: 0.95

#### 報酬設計の制約
```python
reward_scales = {
    "tracking_lin_vel": 1.0,     # 速度追従（平地前提）
    "tracking_ang_vel": 0.2,     # 旋回追従
    "lin_vel_z": -1.0,           # Z軸速度ペナルティ
    "base_height": -50.0,        # 高さ維持（平地前提）
    "action_rate": -0.005,       # アクション変化ペナルティ
    "similar_to_default": -0.1,  # デフォルト姿勢維持
}
# -> 地形適応に関する報酬なし
```

### 4.3 システム性能の課題

#### 計算効率
- **極低FPS**: 0.62 FPS平均（リアルタイムの1/80）
- **長い初期化時間**: 6分の準備時間
- **CPUバックエンド制約**: GPUが利用できない環境

#### スケーラビリティ
- 1環境でも低性能
- 大規模評価（複数試行）が困難
- リアルタイム制御が不可能

## 5. 考察

### 5.1 汎化性能に関する知見

#### 訓練環境の根本的制約
go2_train.pyで確認された訓練環境の制約が汎化失敗の根本原因：

1. **完全な平地環境**: plane.urdfのみで訓練、地形多様性ゼロ
2. **地形情報の完全欠如**: 45次元観測に高さマップや接触情報なし
3. **平地特化報酬**: base_heightペナルティ(-50.0)が地形適応を阻害
4. **固定コマンド**: 0.5m/s前進のみ、地形に応じた速度調整なし

#### ロボット学習の根本的課題
本実験結果は、Go2訓練フレームワークの根本的問題を明確化：

- **単一環境訓練の限界**: plane.urdfのみでは汎化不可能
- **観測設計の不十分性**: 地形適応に必要な情報が欠如
- **報酬関数のミスマッチ**: 平地特化報酬が地形での失敗を助長

### 5.2 技術的洞察

#### 訓練と評価のギャップ分析
```
訓練時の観測 (go2_env.py):
✓ 実際のロボット状態 (base_ang_vel, dof_pos, dof_vel)
✓ 正しい重力方向 (projected_gravity)
✓ 適切なコマンド (0.5m/s 前進)
✓ アクション履歴 (self.actions)

評価時の観測 (評価ツール):
✗ 全てゼロのダミー観測
✗ 固定重力ベクトル [0,0,-1]
✗ コマンドゼロ
✗ アクション履歴なし
```

#### 根本的なアーキテクチャの問題
1. **単一環境設計**: plane.urdfのみでは地形適応能力が育たない
2. **観測空間の不完全性**: 地形情報、接触情報の欠如
3. **報酬関数の偏り**: 平地特化で地形適応を阻害
4. **固定的コマンド**: 地形に応じた柔軟な制御が不可能

## 6. 改善提案

### 6.1 短期的改善策（1-2週間）

#### 1. 観測空間の正しい実装
```python
# go2_env.pyと同様の観測を評価ツールでも使用
def create_proper_observation(robot):
    # 訓練時と同じ観測構造を再現
    base_ang_vel = robot.get_ang()[0]  # 実際の角速度
    projected_gravity = get_projected_gravity(robot)  # 正しい重力方向
    commands = torch.tensor([0.5, 0.0, 0.0])  # 訓練時と同じコマンド
    dof_pos = robot.get_dofs_position(motor_dof_idx)
    dof_vel = robot.get_dofs_velocity(motor_dof_idx)
    
    obs = torch.cat([
        base_ang_vel * obs_scales["ang_vel"],        # 3
        projected_gravity,                           # 3
        commands * commands_scale,                   # 3
        (dof_pos - default_dof_pos) * obs_scales["dof_pos"], # 12
        dof_vel * obs_scales["dof_vel"],             # 12
        last_actions,                                # 12
    ])
    return obs  # 45次元
```

#### 2. 制御パラメータの正しい適用
- **アクションスケール**: 0.25を評価時も適用
- **コマンド速度**: 訓練時と同じ[0.5, 0.0, 0.0]を設定
- **PDゲイン**: 訓練時と同じkp=20.0, kv=0.5を維持
- **初期姿勢**: 訓練時と同じbase_init_pos=[0,0,0.42]を使用

#### 3. 評価プロトコルの改善
- 複数試行による統計的評価（最低10回）
- 異なる初期位置での評価
- 段階的成功基準（1m, 2m, 4m, 8m）

### 6.2 中期的改善策（1-2ヶ月）

#### 1. go2_train.pyの地形対応改修
```python
# 平地のみから地形ランダム化へ変更
# 現在: self.scene.add_entity(gs.morphs.URDF(file="urdf/plane/plane.urdf"))

# 改善案:
terrain_types = ["flat", "steps_low", "steps_med", "slopes_low"]
terrain = self.scene.add_entity(
    gs.morphs.Terrain(
        terrain_type=random.choice(terrain_types),
        subterrain_parameters=terrain_configs
    )
)
```

#### 2. ドメイン適応手法
- **Progressive Domain Adaptation**: 段階的な環境複雑化
- **Adversarial Domain Adaptation**: 環境変化に頑健なポリシー
- **Meta-Learning**: 新環境への高速適応

#### 3. システム性能の最適化
- GPU計算環境への移行
- 並列シミュレーションの実装
- 効率的なレンダリング設定

### 6.3 長期的改善策（3-6ヶ月）

#### 1. 階層的制御アーキテクチャ
```
High-Level Controller:
- 地形認識・分類
- 経路計画
- 歩容選択

Low-Level Controller:
- 関節トルク制御
- バランス制御
- 接触力制御
```

#### 2. 強化学習アルゴリズムの改良
- **Multi-Task Learning**: 複数地形での同時学習
- **Curriculum Learning**: 段階的難易度上昇
- **Sim-to-Real Transfer**: 実機への転移学習

#### 3. 包括的評価フレームワーク
- 標準ベンチマークスイートの構築
- 多様な地形タイプでの評価
- 実機実験との比較検証

## 7. 実験の限界と制約

### 7.1 技術的制約
- **計算資源**: CPU環境による性能制限
- **シミュレーション精度**: 物理モデルの近似誤差
- **観測実装**: 実際のセンサー情報の不完全な模倣

### 7.2 実験設計の制約
- **単一試行**: 統計的有意性の欠如
- **限定的地形**: 1種類の地形のみでの評価
- **固定パラメータ**: 制御パラメータの最適化未実施

### 7.3 ポリシー評価の制約
- **訓練詳細不明**: 元のポリシーの訓練条件が不明
- **適応機能なし**: 評価時の学習・適応メカニズムなし
- **実機検証なし**: シミュレーション結果の実世界での妥当性未確認

## 8. 今後の研究方向

### 8.1 immediate Actions（今後1週間）
1. **基本機能確認**: 平地でのシンプル歩行テスト
2. **観測改良**: 実際のロボット状態を含む観測実装
3. **制御調整**: より適切なPD制御パラメータの探索

### 8.2 Short-term Goals（今後1ヶ月）
1. **段階的評価**: レベル1段差（5cm）での評価
2. **複数試行**: 統計的に有意な評価の実施
3. **性能最適化**: GPU環境での高速化

### 8.3 Long-term Vision（今後6ヶ月）
1. **汎化性能の向上**: 多地形対応ポリシーの開発
2. **実機実験**: シミュレーション結果の実世界検証
3. **標準化**: 地形評価の標準ベンチマーク構築

## 9. 結論

### 9.1 主要な発見
1. **完全な汎化失敗**: 学習済みポリシーは段差地形に全く適応できない
2. **観測設計の重要性**: 適切な状態表現なしでは制御不可能
3. **システム統合の課題**: 実験フレームワーク自体の改善が必要

### 9.2 重要な教訓
- **単一環境訓練の限界**: 多様な環境での訓練が不可欠
- **実装詳細の重要性**: 観測・制御の詳細設計が成功を左右
- **段階的評価の必要性**: 複雑な評価前の基礎確認が重要

### 9.3 研究への貢献
本実験は失敗に終わったが、以下の価値ある知見を提供：
- 地形汎化評価の具体的な実装方法
- よくある失敗パターンとその原因分析
- 今後の研究に向けた具体的な改善方向

### 9.4 最終推奨事項
1. **観測空間の即座の改良**: 実際のロボット状態の利用
2. **段階的難易度設定**: 平地 → 小段差 → 大段差の順次評価
3. **統計的評価**: 単一試行ではなく複数回の評価実施
4. **システム最適化**: GPU環境での性能向上

---

**レポート作成日**: 2024年7月31日  
**Genesis バージョン**: v0.2.1  
**実験コード**: terrain_evaluation_policy_final.py
